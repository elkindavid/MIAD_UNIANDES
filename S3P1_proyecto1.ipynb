{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2023/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 1 - Predicción de precios de vehículos usados\n",
    "\n",
    "En este proyecto podrán poner en práctica sus conocimientos sobre modelos predictivos basados en árboles y ensambles, y sobre la disponibilización de modelos. Para su desasrrollo tengan en cuenta las instrucciones dadas en la \"Guía del proyecto 1: Predicción de precios de vehículos usados\".\n",
    "\n",
    "**Entrega**: La entrega del proyecto deberán realizarla durante la semana 4. Sin embargo, es importante que avancen en la semana 3 en el modelado del problema y en parte del informe, tal y como se les indicó en la guía.\n",
    "\n",
    "Para hacer la entrega, deberán adjuntar el informe autocontenido en PDF a la actividad de entrega del proyecto que encontrarán en la semana 4, y subir el archivo de predicciones a la [competencia de Kaggle](https://www.kaggle.com/t/b8be43cf89c540bfaf3831f2c8506614)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos para la predicción de precios de vehículos usados\n",
    "\n",
    "En este proyecto se usará el conjunto de datos de Car Listings de Kaggle, donde cada observación representa el precio de un automóvil teniendo en cuenta distintas variables como: año, marca, modelo, entre otras. El objetivo es predecir el precio del automóvil. Para más detalles puede visitar el siguiente enlace: [datos](https://www.kaggle.com/jpayne/852k-used-car-listings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo predicción conjunto de test para envío a Kaggle\n",
    "\n",
    "En esta sección encontrarán el formato en el que deben guardar los resultados de la predicción para que puedan subirlos a la competencia en Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rúbrica de Calificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La calificación estará distribuida de la siguiente manera: \n",
    "\n",
    "**Competencia (20 puntos)**\n",
    "\n",
    "Para cada grupo la calificación será proporcional al lugar que ocupe en el ranking de la competencia. Los grupos que ocupen los primeros 5 puestos obtendrán 20 puntos, a los siguientes 5 grupos se le restarán 2 puntos y así sucesivamente. Es decir: posiciones del 1 al 5 obtendrán 20 puntos, posiciones del 6 al 10 obtendrán 18 puntos, posiciones del 11 a 15 obtendrán 16 puntos y así sucesivamente.\n",
    "\n",
    "**Informe (80 puntos)**\n",
    "\n",
    "La calificación del informe se realizará con base en los siguientes criterios (recuerden que deben dar cuenta de cada uno de los aspectos mencionados en los criterios por medio de pantallazos o fracciones de código):\n",
    "\n",
    "<ins>Preprocesamiento de datos (10 puntos)</ins>\n",
    "\n",
    "* Los datos de entrenamiento se dividen en datos de entrenamiento y validación. Si decidieron preprocesar los datos (estandarizar, normalizar, imputar valores, etc), estos son correctamente preprocesados al ajustar sobre los datos de entrenamiento (.fit_transform()) y al transformar los datos del set de validación (.transform()). (10 puntos)\n",
    "\n",
    "<ins>Calibración del modelo (15 puntos)</ins>\n",
    "\n",
    "* Se calibran los parámetros que se consideren pertinentes del modelo de clasificación seleccionado. (5 puntos)\n",
    "\n",
    "* Se justifica el método seleccionado de calibración. (5 puntos)\n",
    "\n",
    "* Se analizan los valores calibrados de cada parámetro y se explica cómo afectan el modelo. (5 puntos)\n",
    "\n",
    "<ins>Entrenamiento del modelo (15 puntos)</ins>\n",
    "\n",
    "* Se entrena el modelo de regresión escogido con los datos del set de entrenamiento preprocesados y los parámetros óptimos. (5 puntos)\n",
    "\n",
    "* Se presenta el desempeño del modelo en los datos de validación con al menos una métrica de desempeño. (5 puntos)\n",
    "\n",
    "* Se justifica la selección del modelo correctamente. (5 puntos)\n",
    "\n",
    "<ins>Disponibilización del modelo (30 puntos)</ins>\n",
    "\n",
    "* Se disponibiliza el modelo en una API alojada en un servicio en la nube. (20 puntos)\n",
    "\n",
    "* Se hacen las predicciones sobre el valor del automóvil en al menos dos observaciones del set de validación. (10 puntos)\n",
    "\n",
    "<ins>Conclusiones (10 puntos)</ins>\n",
    "\n",
    "* Se presentan conclusiones claras y concisas sobre el desarrollo y los resultados del proyecto. (10 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import all what you need for machine learning\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn import ensemble\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, ElasticNet\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "import multiprocessing\n",
    "import math\n",
    "from statistics import mean\n",
    "import joblib\n",
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "# Global settings\n",
    "n_jobs = -1 # This parameter conrols the parallel processing. -1 means using all processors.\n",
    "random_state = 42 # This parameter controls the randomness of the data. Using some int value to get same results everytime this code is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './Data/dataTrain_carListings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Carga de datos de archivo .csv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataTraining \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Data/dataTrain_carListings.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m dataTesting \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Data/dataTest_carListings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m dataTraining\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/dataTrain_carListings.csv'"
     ]
    }
   ],
   "source": [
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('./Data/dataTrain_carListings.csv')\n",
    "dataTesting = pd.read_csv('./Data/dataTest_carListings.csv', index_col=0)\n",
    "data = dataTraining\n",
    "test = dataTesting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis Exploratorio de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización datos de entrenamiento\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols=data.select_dtypes(include=['object']).columns\n",
    "categorical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols=data.select_dtypes(exclude=['object']).columns\n",
    "numerical_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data['Make'].value_counts()).reset_index().rename(columns={'index':'make','Make': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de carros por marca\n",
    "plt.figure(figsize=(8,8))\n",
    "plot = sns.barplot(y='make',x='count',data=df)\n",
    "plot=plt.setp(plot.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(data['State'].value_counts()).reset_index().rename(columns={'index':'state','State': 'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de carros por state\n",
    "plt.figure(figsize=(8,8))\n",
    "plot = sns.barplot(y='state',x='count',data=df2)\n",
    "plot=plt.setp(plot.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribución del Precio de los carros\n",
    "sns.distplot(data['Price'],kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificación de las variables categoricas\n",
    "data['State'] = pd.factorize(data.State)[0]\n",
    "data['Model'] = pd.factorize(data.Model)[0]\n",
    "data['Make'] = pd.factorize(data.Make)[0]\n",
    "\n",
    "test['State'] = pd.factorize(test.State)[0]\n",
    "test['Model'] = pd.factorize(test.Model)[0]\n",
    "test['Make'] = pd.factorize(test.Make)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = data[['Price','Mileage','Year']]\n",
    "df = data\n",
    "filas = math.ceil(len(df.columns) / 3)\n",
    "fig, a = plt.subplots(filas,3,figsize=(20, 10))\n",
    "\n",
    "a = a.ravel()\n",
    "titles = df.columns\n",
    "\n",
    "for i, ax in enumerate(a):\n",
    "    \n",
    "    if i < len(df.columns):\n",
    "        ax.hist(df.iloc[:,i])\n",
    "        ax.set_title(titles[i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = data[['Price','Mileage','Year']]\n",
    "df = data\n",
    "filas = math.ceil(len(df.columns) / 3)\n",
    "fig, a = plt.subplots(filas,3,figsize=(20, 10))\n",
    "\n",
    "a = a.ravel()\n",
    "titles = df.columns\n",
    "\n",
    "for i, ax in enumerate(a):\n",
    "    \n",
    "    if i < len(df.columns):\n",
    "        ax.boxplot(df.iloc[:,i])\n",
    "        ax.set_title(titles[i])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cantidad Datos:',len(data))\n",
    "\n",
    "# Control de Outliers\n",
    "Q1 = data['Price'].quantile(0.25)\n",
    "print('Primer Cuartil:', Q1)\n",
    "\n",
    "Q3 = data['Price'].quantile(0.75)\n",
    "print('Tercer Cuartil:', Q3)\n",
    "\n",
    "IQR = Q3 - Q1\n",
    "print('Rango Intercuartil:', IQR)\n",
    "\n",
    "Mediana = data['Price'].median()\n",
    "print('Mediana:', Mediana)\n",
    "\n",
    "Valor_Minimo = data['Price'].min()\n",
    "print('Valor Mínimo:', Valor_Minimo)\n",
    "\n",
    "Valor_Maximo = data['Price'].max()\n",
    "print('Valor Máximo:', Valor_Maximo)\n",
    "\n",
    "BI_Calculado = (Q1 - 1.5 * IQR)\n",
    "print('BI_Calculado: \\n', BI_Calculado)\n",
    "\n",
    "BS_Calculado = (Q3 + 1.5 * IQR)\n",
    "print('BS_Calculado: \\n', BS_Calculado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubicacion_outliers = (data['Price'] < BI_Calculado) | (data['Price'] > BS_Calculado)\n",
    "outliers = data[ubicacion_outliers]\n",
    "print('\\n Lista de Outliers \\n', outliers)\n",
    "\n",
    "ubicacion_sin_out = (data['Price'] >= BI_Calculado) & (data['Price'] <= BS_Calculado)\n",
    "data = data[ubicacion_sin_out]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total = len(data)\n",
    "data['Por_Make'] = 0\n",
    "for i in data['Make'].unique():\n",
    "    data.loc[data['Make'] == i,'Por_Make'] = len(data[data['Make'] == i])/total\n",
    "\n",
    "data['Por_State'] = 0\n",
    "for i in data['State'].unique():\n",
    "    data.loc[data['State'] == i,'Por_State'] = len(data[data['State'] == i])/total\n",
    "    \n",
    "data.Por_Make.value_counts().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(test)\n",
    "test['Por_Make'] = 0\n",
    "for i in test['Make'].unique():\n",
    "    test.loc[test['Make'] == i,'Por_Make'] = len(test[test['Make'] == i])/total\n",
    "\n",
    "test['Por_State'] = 0\n",
    "for i in test['State'].unique():\n",
    "    test.loc[test['State'] == i,'Por_State'] = len(test[test['State'] == i])/total\n",
    "    \n",
    "test.Por_Make.value_counts().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de variables numéricas (X)\n",
    "# X = data.drop(['Price','State','Model','Make'], axis=1)\n",
    "# test = test.drop(['State','Model','Make'], axis=1)\n",
    "\n",
    "X = data.drop(['Price','Make','State','Model'], axis=1)\n",
    "test = test.drop(['Make','State','Model'], axis=1)\n",
    "\n",
    "X['YxM'] = X['Year'] * X['Mileage']\n",
    "test['YxM'] = test['Year'] * test['Mileage']\n",
    "\n",
    "# Definición variable de interés binaria (y)\n",
    "y = data['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y) en set de entrenamiento y test usandola función train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator = LinearRegression()\n",
    "\n",
    "# selector = RFE(estimator, n_features_to_select=20, step=5) #Se define step=1, pero lo puede cambiar para que sea más rápido\n",
    "# selector = selector.fit(X_train, y_train) # esto puede tardar algunos minutos\n",
    "\n",
    "# selected_vars = X_train.iloc[:,selector.support_].columns\n",
    "# selected_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train[selected_vars]\n",
    "# X_test = X_test[selected_vars]\n",
    "# test = test[selected_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "X = scaler.fit_transform(X)\n",
    "test = scaler.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_scores = [] # To store model scores\n",
    "\n",
    "def rmse(model):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return mean_squared_error(y_test, y_pred, squared= False) # squared= False > returns Root Mean Square Error                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_regression = make_pipeline(LinearRegression())\n",
    "# score = rmse(linear_regression)\n",
    "\n",
    "# models_scores.append(['LinearRegression', score])\n",
    "# print(f'LinearRegression Score= {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state= random_state))\n",
    "\n",
    "# score = rmse(lasso)\n",
    "# models_scores.append(['Lasso', score])\n",
    "# print(f'Lasso Score= {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elastic_net = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio= .9, random_state= random_state))\n",
    "\n",
    "# score = rmse(elastic_net)\n",
    "# models_scores.append(['ElasticNet', score])\n",
    "# print(f'ElasticNet Score= {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibración de Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gradient Boosting\n",
    "# # Validación empleando k-cross-validation y neg_root_mean_squared_error\n",
    "# # ==============================================================================\n",
    "# train_scores = []\n",
    "# cv_scores    = []\n",
    "\n",
    "# # Valores evaluados\n",
    "# estimator_range = range(1, 500, 25)\n",
    "\n",
    "# # Bucle para entrenar un modelo con cada valor de n_estimators y extraer su error\n",
    "# # de entrenamiento y de k-cross-validation.\n",
    "# for n_estimators in estimator_range:\n",
    "    \n",
    "#     modelo = GradientBoostingRegressor(\n",
    "#                 n_estimators = n_estimators,\n",
    "#                 loss         = 'ls',\n",
    "#                 max_features = 'auto',\n",
    "#                 random_state = 123\n",
    "#              )\n",
    "    \n",
    "#     # Error de train\n",
    "#     modelo.fit(X_train, y_train)\n",
    "#     predicciones = modelo.predict(X = X_train)\n",
    "#     rmse = mean_squared_error(\n",
    "#             y_true  = y_train,\n",
    "#             y_pred  = predicciones,\n",
    "#             squared = False\n",
    "#            )\n",
    "#     train_scores.append(rmse)\n",
    "    \n",
    "#     # Error de validación cruzada\n",
    "#     scores = cross_val_score(\n",
    "#                 estimator = modelo,\n",
    "#                 X         = X_train,\n",
    "#                 y         = y_train,\n",
    "#                 scoring   = 'neg_root_mean_squared_error',\n",
    "#                 cv        = 5,\n",
    "#                 n_jobs    = multiprocessing.cpu_count() - 1,\n",
    "#              )\n",
    "#     # Se agregan los scores de cross_val_score() y se pasa a positivo\n",
    "#     cv_scores.append(-1*scores.mean())\n",
    "    \n",
    "# # Gráfico con la evolución de los errores\n",
    "# fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "# ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "# ax.plot(estimator_range, cv_scores, label=\"cv scores\")\n",
    "# ax.plot(estimator_range[np.argmin(cv_scores)], min(cv_scores),\n",
    "#         marker='o', color = \"red\", label=\"min score\")\n",
    "# ax.set_ylabel(\"root_mean_squared_error\")\n",
    "# ax.set_xlabel(\"n_estimators\")\n",
    "# ax.set_title(\"Evolución del cv-error vs número árboles\")\n",
    "# plt.legend();\n",
    "# print(f\"Valor óptimo de n_estimators: {estimator_range[np.argmin(cv_scores)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validación empleando k-cross-validation y neg_root_mean_squared_error\n",
    "# # ==============================================================================\n",
    "# resultados = {}\n",
    "\n",
    "# # Valores evaluados\n",
    "# learning_rates = [0.001, 0.01, 0.1]\n",
    "# n_estimators   = [10, 20, 100, 200, 300, 400, 500]\n",
    "\n",
    "\n",
    "# # Bucle para entrenar un modelo con cada combinacion de  learning_rate y n_estimator \n",
    "# # y extraer su error de entrenamiento y k-cross-validation.\n",
    "# for learning_rate in learning_rates:\n",
    "#     train_scores = []\n",
    "#     cv_scores    = []\n",
    "    \n",
    "#     for n_estimator in n_estimators:\n",
    "    \n",
    "#         modelo = GradientBoostingRegressor(\n",
    "#                     n_estimators  = n_estimator,\n",
    "#                     learning_rate = learning_rate,\n",
    "#                     loss          = 'ls',\n",
    "#                     max_features  = 'auto',\n",
    "#                     random_state  = 123\n",
    "#                  )\n",
    "\n",
    "#         # Error de train\n",
    "#         modelo.fit(X_train, y_train)\n",
    "#         predicciones = modelo.predict(X = X_train)\n",
    "#         rmse = mean_squared_error(\n",
    "#                 y_true  = y_train,\n",
    "#                 y_pred  = predicciones,\n",
    "#                 squared = False\n",
    "#                )\n",
    "#         train_scores.append(rmse)\n",
    "\n",
    "#         # Error de validación cruzada\n",
    "#         scores = cross_val_score(\n",
    "#                     estimator = modelo,\n",
    "#                     X         = X_train,\n",
    "#                     y         = y_train,\n",
    "#                     scoring   = 'neg_root_mean_squared_error',\n",
    "#                     cv        = 3,\n",
    "#                     n_jobs    = multiprocessing.cpu_count() - 1\n",
    "#                  )\n",
    "#         # Se agregan los scores de cross_val_score() y se pasa a positivo\n",
    "#         cv_scores.append(-1*scores.mean())\n",
    "        \n",
    "#     resultados[learning_rate] = {'train_scores': train_scores, 'cv_scores': cv_scores}\n",
    "\n",
    "# # Gráfico con la evolución de los errores de entrenamiento\n",
    "# fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 3.84))\n",
    "\n",
    "# for key, value in resultados.items():\n",
    "#     axs[0].plot(n_estimators, value['train_scores'], label=f\"Learning rate {key}\")\n",
    "#     axs[0].set_ylabel(\"root_mean_squared_error\")\n",
    "#     axs[0].set_xlabel(\"n_estimators\")\n",
    "#     axs[0].set_title(\"Evolución del train error vs learning rate\")\n",
    "    \n",
    "#     axs[1].plot(n_estimators, value['cv_scores'], label=f\"Learning rate {key}\")\n",
    "#     axs[1].set_ylabel(\"root_mean_squared_error\")\n",
    "#     axs[1].set_xlabel(\"n_estimators\")\n",
    "#     axs[1].set_title(\"Evolución del cv-error vs learning rate\")\n",
    "#     plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Validación empleando k-cross-validation y neg_root_mean_squared_error\n",
    "# # ==============================================================================\n",
    "# train_scores = []\n",
    "# cv_scores    = []\n",
    "\n",
    "# # Valores evaluados\n",
    "# max_depths = [1, 3, 5, 10, 20]\n",
    "\n",
    "# # Bucle para entrenar un modelo con cada valor de max_depth y extraer su error\n",
    "# # de entrenamiento y de k-cross-validation.\n",
    "# for max_depth in max_depths:\n",
    "    \n",
    "#     modelo = GradientBoostingRegressor(\n",
    "#                 n_estimators = 100,\n",
    "#                 loss         = 'ls',\n",
    "#                 max_depth    = max_depth,\n",
    "#                 max_features = 'auto',\n",
    "#                 random_state = 123\n",
    "#              )\n",
    "    \n",
    "#     # Error de train\n",
    "#     modelo.fit(X_train, y_train)\n",
    "#     predicciones = modelo.predict(X = X_train)\n",
    "#     rmse = mean_squared_error(\n",
    "#             y_true  = y_train,\n",
    "#             y_pred  = predicciones,\n",
    "#             squared = False\n",
    "#            )\n",
    "#     train_scores.append(rmse)\n",
    "    \n",
    "#     # Error de validación cruzada\n",
    "#     scores = cross_val_score(\n",
    "#                 estimator = modelo,\n",
    "#                 X         = X_train,\n",
    "#                 y         = y_train,\n",
    "#                 scoring   = 'neg_root_mean_squared_error',\n",
    "#                 cv        = 5,\n",
    "#                 n_jobs    = multiprocessing.cpu_count() - 1\n",
    "#              )\n",
    "#     # Se agregan los scores de cross_val_score() y se pasa a positivo\n",
    "#     cv_scores.append(-1*scores.mean())\n",
    "    \n",
    "# # Gráfico con la evolución de los errores\n",
    "# fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "# ax.plot(max_depths, train_scores, label=\"train scores\")\n",
    "# ax.plot(max_depths, cv_scores, label=\"cv scores\")\n",
    "# ax.plot(max_depths[np.argmin(cv_scores)], min(cv_scores),\n",
    "#         marker='o', color = \"red\", label=\"min score\")\n",
    "# ax.set_ylabel(\"root_mean_squared_error\")\n",
    "# ax.set_xlabel(\"max_depth\")\n",
    "# ax.set_title(\"Evolución del cv-error vs profundidad árboles\")\n",
    "# plt.legend();\n",
    "# print(f\"Valor óptimo de max_depth: {max_depths[np.argmin(cv_scores)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB Regresor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM Regresor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_regressor= GradientBoostingRegressor(n_estimators=476, learning_rate=0.1,\n",
    "                                   max_depth=10, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state = random_state)\n",
    "\n",
    "score = rmse(gradient_boosting_regressor)\n",
    "models_scores.append(['GradientBoostingRegressor', score])\n",
    "print(f'GradientBoostingRegressor Score= {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_regressor= xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "#                              learning_rate=0.05, max_depth=3, \n",
    "#                              min_child_weight=1.7817, n_estimators=2200,\n",
    "#                              reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "#                              subsample=0.5213,verbosity=0, nthread = -1, random_state = random_state)\n",
    "# score = rmse(xgb_regressor)\n",
    "# models_scores.append(['XGBRegressor', score])\n",
    "# print(f'XGBRegressor Score= {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm_regressor= lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "#                               learning_rate=0.05, n_estimators=720,\n",
    "#                               max_bin = 55, bagging_fraction = 0.8,\n",
    "#                               bagging_freq = 5, feature_fraction = 0.2319,\n",
    "#                               feature_fraction_seed=9, bagging_seed=9,\n",
    "#                               min_data_in_leaf =6, min_sum_hessian_in_leaf = 11,random_state = random_state)\n",
    "\n",
    "# score = rmse(lgbm_regressor)\n",
    "# models_scores.append(['LGBMRegressor', score])\n",
    "# print(f'LGBMRegressor Score= {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimators = [ ('lgbm_regressor', lgbm_regressor) , ('GradientBoostingRegressor', gradient_boosting_regressor),('xgb_regressor', xgb_regressor) ]\n",
    "\n",
    "# stack = StackingRegressor(estimators=estimators, final_estimator= lasso, cv= 5, n_jobs= n_jobs, passthrough = True)\n",
    "\n",
    "# stack.fit(X_train, y_train)\n",
    "\n",
    "# pred = stack.predict(X_test)\n",
    "\n",
    "# rmse_val = mean_squared_error(y_test, pred, squared= False) # squared= False > returns Root Mean Square Error    \n",
    "# models_scores.append(['Stacking', rmse_val])\n",
    "# print(f'rmse= {rmse_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción del conjunto de test - acá se genera un número aleatorio como ejemplo\n",
    "y_pred_test = gradient_boosting_regressor.predict(test)\n",
    "y_pred = pd.DataFrame(y_pred_test, index=dataTesting.index, columns=['Price'])\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar predicciones en formato exigido en la competencia de kaggle\n",
    "y_pred.to_csv('test_submission.csv', index_label='ID')\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Microservicios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar modelo a archivo binario .pkl\n",
    "joblib.dump(gradient_boosting_regressor, 'model_deployment/car_price_reg.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar modelo y predicción\n",
    "from model_deployment.m09_model_deployment import predict\n",
    "\n",
    "# Predicción de probabilidad de que un link sea phishing\n",
    "predict(0,0,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disponibilizar modelo con Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "from flask import Flask\n",
    "from flask_restx import Api, Resource, fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición aplicación Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Definición API Flask\n",
    "api = Api(\n",
    "    app, \n",
    "    version='1.0', \n",
    "    title='Car Price Prediction API',\n",
    "    description='Car Price Prediction API')\n",
    "\n",
    "ns = api.namespace('predict', \n",
    "     description='Car Price Regression')\n",
    "\n",
    "# Definición argumentos o parámetros de la API\n",
    "parser = api.parser()\n",
    "# \tYear\tMileage\tState\tMake\n",
    "parser.add_argument(\n",
    "    'YEAR', \n",
    "    type=str, \n",
    "    required=True, \n",
    "    help='Año de Fabricación', \n",
    "    location='args')\n",
    "\n",
    "parser.add_argument(\n",
    "    'MILEAGE', \n",
    "    type=str, \n",
    "    required=True, \n",
    "    help='Kilometraje', \n",
    "    location='args')\n",
    "\n",
    "parser.add_argument(\n",
    "    'STATE', \n",
    "    type=str, \n",
    "    required=True, \n",
    "    help='Estado', \n",
    "    location='args')\n",
    "\n",
    "parser.add_argument(\n",
    "    'MAKE', \n",
    "    type=str, \n",
    "    required=True, \n",
    "    help='Fabricante', \n",
    "    location='args')\n",
    "\n",
    "resource_fields = api.model('Resource', {\n",
    "    'result': fields.String,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase para disponibilización\n",
    "@ns.route('/')\n",
    "class CarPriceApi(Resource):\n",
    "\n",
    "    @api.doc(parser=parser)\n",
    "    @api.marshal_with(resource_fields)\n",
    "    def get(self):\n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        return {\n",
    "         \"result\": predict(args['YEAR','MILEAGE','STATE','MAKE'])\n",
    "        }, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución de la aplicación que disponibiliza el modelo de manera local en el puerto 5000\n",
    "app.run(debug=True, use_reloader=False, host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
